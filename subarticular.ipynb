{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTConfig, TrainingArguments, Trainer, DefaultDataCollator, get_cosine_schedule_with_warmup, SwinConfig, SwinModel, AutoFeatureExtractor, SwinForImageClassification\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from fastai.vision.all import *\n",
    "import PIL.Image as Image\n",
    "!pip install segmentation_models_pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SEED = 777\n",
    "FOLDS = [1,2,3,4,5]\n",
    "PATCH_SIZE = 512\n",
    "patch_size = 128\n",
    "Lmax = 36\n",
    "BS = 16\n",
    "EPOCHS = 10\n",
    "patch_size=128\n",
    "CV=5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2437c92c903aa581"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9972feb99bc4e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_meta_f = pd.read_csv('./datasets/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\n",
    "df_meta_f.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25c136ce1efcf985"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_coor = pd.read_csv('./datasets/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv')\n",
    "df_coor.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "678c8b2c66113cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./datasets/rsna-2024-lumbar-spine-degenerative-classification/train.csv\")\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65986c2fac7af230"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gdf = df = df_coor[\n",
    "    df_coor.condition.isin(\n",
    "        [\n",
    "            'Left Subarticular Stenosis',\n",
    "            'Right Subarticular Stenosis'\n",
    "        ]\n",
    "    )\n",
    "][['study_id','series_id','instance_number','level','x','y']].drop_duplicates()\n",
    "\n",
    "gdf_new = gdf.groupby(['study_id','series_id','level','instance_number'],as_index=False).agg({'x':'mean','y':'mean'})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b770b872c07924d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Localizing ROI for Subarticular Dataset Segmentation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2213f7be99f6bdd6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class AxialSegDataset(Dataset):\n",
    "    def __init__(self, df, VALID=False, alpha=0):\n",
    "        self.data = df\n",
    "        self.VALID = VALID\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "\n",
    "#         centers = torch.as_tensor([x for x in row[coor]]).view(5,2).float()\n",
    "        centers = torch.as_tensor(np.array([x for x in row[['x','y']]])).view(1,2).float()\n",
    "        \n",
    "        sample = './datasets/train_images/'\n",
    "        sample1 = sample+str(row['study_id'])+'/'+str(row['series_id'])+'/'+str(row['instance_number'])+'.dcm'\n",
    "        print(row)\n",
    "        image = pydicom.dcmread(sample1).pixel_array\n",
    "        H,W = image.shape\n",
    "#       By plane resizing I've been distorting the proportions\n",
    "        if H > W:\n",
    "            d = W\n",
    "            if not self.VALID:\n",
    "                h = int((H - d)*(.5 + self.alpha*(.5 - np.random.rand())))\n",
    "            else:\n",
    "                h = (H - d)//2\n",
    "            image = image[h:h+d]\n",
    "            centers[:,1] -= h\n",
    "            H = W\n",
    "        elif H < W:\n",
    "            d = H\n",
    "            if not self.VALID:\n",
    "                w = int((W - d)*(.5 + self.alpha*(.5 - np.random.rand())))\n",
    "            else:\n",
    "                w = (W - d)//2\n",
    "            image = image[:,w:w+d]\n",
    "            centers[:,0] -= w\n",
    "            W = H\n",
    "        image = cv2.resize(image,(PATCH_SIZE,PATCH_SIZE))\n",
    "        image = torch.as_tensor(image/np.max(image)).unsqueeze(0).float()\n",
    "        \n",
    "#         label = torch.as_tensor([labels[x] for x in row[target]])\n",
    "        \n",
    "        centers[:,0] = centers[:,0]*PATCH_SIZE/W\n",
    "        centers[:,1] = centers[:,1]*PATCH_SIZE/H\n",
    "\n",
    "        if not self.VALID: image,centers = augment_image_and_centers(image,centers,self.alpha)\n",
    "\n",
    "        return image.to(device),centers.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f541a8ee7e994569"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idx_map = torch.stack([torch.arange(PATCH_SIZE)]*PATCH_SIZE).to(device)\n",
    "idx_map = torch.stack([idx_map,idx_map.T]).view(1,1,2,PATCH_SIZE,PATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "468b07f740f297a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class myLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=.5\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def clone(self):\n",
    "        return myLoss(self.alpha)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            y,# Predictions\n",
    "            t # Targets\n",
    "        ):\n",
    "#         with torch.cuda.amp.autocast():\n",
    "        mask_pred = y.to(device)\n",
    "        mask_true = t.to(device)\n",
    "#         print(\"mask_pred.shape\",mask_pred.shape)\n",
    "#         print(\"mask_true.shape\",mask_true.shape)\n",
    "#       The heatmap Loss as the distance between the predicted Normal and the ideal one\n",
    "#       Let's define the ideal heatmaps as the Normal distributions\n",
    "#       centered on the diagnostic centers with s2 = PATCH_SIZE/8\n",
    "        s2 = s2 = torch.as_tensor([PATCH_SIZE/8]*2)\n",
    "#       Then the corresponding alphas and normalization constants would be\n",
    "        A = -1/(2*s2).to(device)\n",
    "        K = 1/torch.sqrt(2*math.pi*s2).to(device)\n",
    "#       Predicted heatmaps rescaling\n",
    "        mask_pred = mask_pred*K.view(1,2,1,1)\n",
    "#         print(\"mask_pred2 shape\",mask_pred.shape)\n",
    "#       Ideal heatmaps\n",
    "        mask = idx_map - mask_true.view(-1,1,2,1,1)\n",
    "#         mask = torch.exp((A.view(-1,2,1,1,1)*mask*mask).sum(2))*K.view(-1,2,1,1)\n",
    "        mask = torch.exp((A.view(1,2,1,1,1)*mask*mask).sum(2))*K.view(1,2,1,1)\n",
    "#             mask = torch.exp((A.view(-1,1,1,1,1)*mask*mask).sum(2))*K.view(1,2,1,1)\n",
    "#             mask = idx_map - mask_true.view(-1,1,2,1,1)\n",
    "#             mask = torch.exp((A.view(1,2,1,1,1)*mask*mask).sum(2))*K.view(1,2,1,1)\n",
    "#         print(torch.exp((A.unsqueeze(0).view(1,1,2,1,1).expand(32,1,2,512,512)*mask*mask)).shape)\n",
    "#             mask = torch.exp((A.unsqueeze(0).view(1,1,2,1,1).expand(BS,1,2,512,512)*mask*mask).sum(2))*K.unsqueeze(0).view(1,1,2,1,1).expand(BS,1,2,512,512)\n",
    "#       Distance\n",
    "#         print(\"mask shape\",mask.shape)\n",
    "#         print(\"mask_pred shape\",mask_pred.shape)\n",
    "        D = 1 - ((mask*mask_pred).sum())**2/((mask*mask).sum()*(mask_pred*mask_pred).sum())\n",
    "\n",
    "        return D"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "678c6c30dd6fcc97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class AxialUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myUNet, self).__init__()\n",
    "\n",
    "        self.UNet = smp.Unet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            classes=2,\n",
    "            in_channels=1\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self,X):\n",
    "        x = self.UNet(X)\n",
    "#       MinMaxScaling along the class plane to generate a heatmap\n",
    "        min_values = x.view(-1,2,PATCH_SIZE*PATCH_SIZE).min(-1)[0].view(-1,2,1,1) # Bug, I've been MinMaxScaling with the wrong values\n",
    "        max_values = x.view(-1,2,PATCH_SIZE*PATCH_SIZE).max(-1)[0].view(-1,2,1,1)\n",
    "        x = (x - min_values)/(max_values - min_values)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "258238ab1c830415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Segmentation Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18310e7b9f62a73d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def nt(nmin,nmax,tcur,tmax):\n",
    "    return (nmax - .5*(nmax-nmin)*(1+np.cos(tcur*np.pi/tmax))).astype(np.float32)\n",
    "\n",
    "plt.plot(nt(0,1,np.arange(10),10))\n",
    "plt.show()\n",
    "\n",
    "# callback to update alpha during training\n",
    "def cb(self):\n",
    "    alpha = torch.as_tensor(nt(.25,1,learn.train_iter,10*n_iter))\n",
    "    learn.dls.train_ds.alpha = alpha\n",
    "alpha_cb = Callback(before_batch=cb)#\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "\n",
    "    tdf = gdf_new[gdf_new['fold'] != fold]\n",
    "    vdf = gdf_new[gdf_new['fold'] == fold]\n",
    "    \n",
    "    tds = AxialSegDataset(tdf)\n",
    "    vds = AxialSegDataset(vdf,VALID=True)\n",
    "    tdl = torch.utils.data.DataLoader(tds, batch_size=BS, shuffle=True, drop_last=True)\n",
    "    vdl = torch.utils.data.DataLoader(vds, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    dls = DataLoaders(tdl,vdl)\n",
    "    \n",
    "    n_iter = len(tds)//BS\n",
    "    \n",
    "    model = AxialUnet()\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        lr=5e-4,\n",
    "        loss_func=myLoss(alpha=0.5),\n",
    "        cbs=[\n",
    "            ShowGraphCallback(),\n",
    "            alpha_cb\n",
    "        ]\n",
    "    )\n",
    "    learn.fit_one_cycle(10)\n",
    "    #   learn.fit(SEG['EPOCHS'])\n",
    "    torch.save(model,'SEG_subaritcular_'+str(fold))\n",
    "    del tdl,vdl,dls,model,learn\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b158c7477f4e97b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "axial_seg_models = []\n",
    "for i in range(1,6):\n",
    "    axial_seg_models.append(torch.load('./datasets/axial-segmentation-models/SEG_Axial_'+str(i)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbfca9dfe64b3054"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "split_len = train_len/CV\n",
    "split_indices = list(np.rint(np.arange(CV)*split_len).astype(int))+[train_len]\n",
    "for i in range(5):\n",
    "    train.loc[split_indices[i]:split_indices[i+1],'fold']=i+1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a285be5d2f93b7ad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = df_coor[\n",
    "    df_coor.condition.isin(\n",
    "        [\n",
    "            'Left Subarticular Stenosis',\n",
    "            'Right Subarticular Stenosis'\n",
    "        ]\n",
    "    )\n",
    "][['study_id','series_id','instance_number','level']].drop_duplicates()\n",
    "df_min = df.groupby(['study_id','series_id','level'],sort=True).min().reset_index().rename(columns={'instance_number':'instance_number_min'})\n",
    "df_max = df.groupby(['study_id','series_id','level'],sort=True).max().reset_index().rename(columns={'instance_number':'instance_number_max'})\n",
    "df_min['instance_number_max'] = df_max['instance_number_max']\n",
    "df = df_min"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a9e7c0b72f8ed77"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "centers = {}\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    centers[row['study_id']]={}\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    centers[row['study_id']][row['series_id']]={}\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    centers[row['study_id']][row['series_id']][row['level']] = [row['instance_number_min'],row['instance_number_max']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d8e598c735cdb69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = df.groupby(['study_id','series_id']).count().reset_index()[['study_id','series_id']]\n",
    "df.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4610650f1542c4d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We'll use 0 as missing value\n",
    "v = np.zeros((len(df),10)).astype(int)\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    for level in centers[row['study_id']][row['series_id']]:\n",
    "        v_min,v_max = centers[row['study_id']][row['series_id']][level]\n",
    "        v[i,{'L1/L2':0,'L2/L3':2,'L3/L4':4,'L4/L5':6,'L5/S1':8}[level]] = v_min\n",
    "        v[i,{'L1/L2':1,'L2/L3':3,'L3/L4':5,'L4/L5':7,'L5/S1':9}[level]] = v_max\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f255533473f47f7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[[\n",
    "    'L1L2_min','L1L2_max',\n",
    "    'L2L3_min','L2L3_max',\n",
    "    'L3L4_min','L3L4_max',\n",
    "    'L4L5_min','L4L5_max',\n",
    "    'L5S1_min','L5S1_max'\n",
    "]] = v\n",
    "df[(df[[\n",
    "    'L1L2_min','L1L2_max',\n",
    "    'L2L3_min','L2L3_max',\n",
    "    'L3L4_min','L3L4_max',\n",
    "    'L4L5_min','L4L5_max',\n",
    "    'L5S1_min','L5S1_max'\n",
    "]] == 0).sum(1)>0].reset_index(drop=True).tail()\n",
    "\n",
    "df['flip'] = False\n",
    "fdf = df.copy()\n",
    "fdf['flip'] = True\n",
    "df = pd.concat([df,fdf]).reset_index(drop=True)\n",
    "df.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "761b61307da915d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "level_train = df.merge(train[['study_id','fold']],left_on='study_id',right_on='study_id')\n",
    "level_train.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54f899614a9af28f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Coordinates from Segmentation Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "630fd6e447ec93fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3aedd3c7ea79622b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "coords = {}\n",
    "for i in range(CV):\n",
    "    coords[i+1]={}\n",
    "for i in range(len(level_train)):\n",
    "    study_id = level_train.iloc[i]['study_id']\n",
    "    series_id = level_train.iloc[i]['series_id']\n",
    "    f = level_train.iloc[i]['fold']\n",
    "    if study_id not in coords[f]:\n",
    "        coords[f][study_id] = {}\n",
    "    if series_id not in coords[f][study_id]:\n",
    "        coords[f][study_id][series_id]=[]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8394fcc72ce7bb75"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def input_image_resizing(image):\n",
    "    H,W = image.shape\n",
    "#       By plane resizing I've been distorting the proportions\n",
    "    if H > W:\n",
    "        d = W\n",
    "        h = int((H - d)*(.5 + .5*(.5 - np.random.rand())))\n",
    "        image = image[h:h+d]\n",
    "        H = W\n",
    "    elif H < W:\n",
    "        d = H\n",
    "        w = int((W - d)*(.5 + .5*(.5 - np.random.rand())))\n",
    "        image = image[:,w:w+d]\n",
    "        W = H\n",
    "    image = cv2.resize(image,(PATCH_SIZE,PATCH_SIZE))\n",
    "    image = torch.as_tensor(image/np.max(image)).unsqueeze(0).float()\n",
    "    return image\n",
    "\n",
    "def create_coords(df,unetModels):\n",
    "    counter = 0\n",
    "    sample = './datasets/train_images/'\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        sample1 = sample+str(int(row['study_id']))+'/'+str(int(row['series_id']))\n",
    "        images = [x.replace('\\\\','/') for x in glob.glob(sample1+'/*.dcm')]\n",
    "#         if(row['study_id']==2091088734):\n",
    "#             print(len(images),sample)\n",
    "        slices = list(np.arange(len(images)))\n",
    "        slices.sort(key=lambda k:int(images[k].split('/')[-1].replace('.dcm','')))\n",
    "        images = np.array(images)[slices]\n",
    "        images = [pydicom.dcmread(img).pixel_array for img in images]\n",
    "#         print(row['study_id'],row['series_id'])\n",
    "        \n",
    "        for img in images:\n",
    "            img = input_image_resizing(img)\n",
    "            OUT = unetModels[0](img.to(device).unsqueeze(0)).cpu().detach()\n",
    "            for j in range(1,len(unetModels)):\n",
    "                OUT+=unetModels[j](img.to(device).unsqueeze(0)).cpu().detach()\n",
    "            OUT/=len(unetModels)\n",
    "    \n",
    "            # cc = []\n",
    "            for k in range(2):\n",
    "                c = (OUT[0,k].unsqueeze(0)*idx_map[0,0].cpu()).sum(-1).sum(-1)\n",
    "                d = OUT[0,k].sum()\n",
    "                c = c/d\n",
    "#                 print(c,type(c),c.shape)\n",
    "#                 cc.append(c)\n",
    "                YY,XX = c.float()\n",
    "            # cc = torch.stack(cc)\n",
    "#             print(cc,cc.shape)\n",
    "#             cc = torch.mean(cc,dim=0)\n",
    "                coords[row['fold']][row['study_id']][row['series_id']].append(torch.tensor(np.array([YY,XX])))\n",
    "        counter+=1\n",
    "        if (counter+1)%100 == 0:\n",
    "            print(\"Progress: {}/{}\".format(counter,len(df)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "177eeec7be9e001c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "create_coords(level_train,axial_seg_models)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "503d4a1d76c77310"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('axial_pred_centers.pkl', 'wb') as f:\n",
    "    pickle.dump(coords, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3580a899dc72d20"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('axial_pred_centers.pkl', 'rb') as f:\n",
    "    coords = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dc2ca1cd1f0cb8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Slice Predictor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a69b22e729f82aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Axial_ViT_Dataset(Dataset):\n",
    "    def __init__(self, df, f, VALID=False, INFERENCE=False, alpha=0):\n",
    "        self.data = df\n",
    "        self.f = f\n",
    "        self.VALID = VALID\n",
    "        self.INFERENCE = INFERENCE\n",
    "        self.resize = torchvision.transforms.Resize((PATCH_SIZE,PATCH_SIZE),antialias=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        \n",
    "        sample = './datasets/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n",
    "        sample1 = sample+str(int(row['study_id']))+'/'+str(int(row['series_id']))\n",
    "\n",
    "        images = [x.replace('\\\\','/') for x in glob.glob(sample1+'/*.dcm')]\n",
    "        slices = list(np.arange(len(images)))\n",
    "        slices.sort(key=lambda k:int(images[k].split('/')[-1].replace('.dcm','')))\n",
    "        instance_numbers = torch.as_tensor([int(x.split('/')[-1].replace('.dcm','')) for x in images])[slices]\n",
    "        images = np.array(images)[slices]\n",
    "        D = len(images)\n",
    "        c = torch.stack(coord[row['fold']][row['study_id']][row['series_id']])\n",
    "#         c = (coord[row['fold']][row['study_id']][row['series_id']]).clone()\n",
    "        \n",
    "#         print(\"c shape:\",c.shape)\n",
    "        c[c < 64] = torch.nan\n",
    "        c[c > 512 - 64] = torch.nan\n",
    "#         print(D,images,slices,row['study_id'],row['series_id'])\n",
    "        if D > Lmax:\n",
    "            slices = np.rint(torch.arange(Lmax)*D/Lmax).long()\n",
    "            images = images[slices]\n",
    "            c = c[slices]\n",
    "            instance_numbers = instance_numbers[slices]\n",
    "        elif D < Lmax:\n",
    "            N = Lmax//D + 1\n",
    "            slices = torch.repeat_interleave(torch.arange(D), N) \n",
    "            slices = slices[np.rint(torch.arange(Lmax)*(D*N)/Lmax).long()]\n",
    "            images = images[slices]\n",
    "            c = c[slices]\n",
    "        \n",
    "        images = [torch.as_tensor(pydicom.dcmread(img).pixel_array.astype('float32')) for img in images]\n",
    "        shapes = [img.shape for img in images]\n",
    "        H,W = np.array(shapes).max(0)\n",
    "\n",
    "        image = torch.concat([torch.nn.functional.pad(\n",
    "            images[k].unsqueeze(0),(\n",
    "                (W - shapes[k][-1])//2,\n",
    "                (W - shapes[k][-1]) - (W - shapes[k][-1])//2,\n",
    "                (H - shapes[k][-2])//2,\n",
    "                (H - shapes[k][-2]) - (H - shapes[k][-2])//2\n",
    "            ),\n",
    "        mode='reflect') for k in range(len(images))]).float()\n",
    "\n",
    "        if H > W:\n",
    "            d = W\n",
    "            h = (H - d)//2\n",
    "            image = image[:,h:h+d]\n",
    "            H = W\n",
    "        elif H < W:\n",
    "            d = H\n",
    "            w = (W - d)//2\n",
    "            image = image[:,:,w:w+d]\n",
    "            W = H\n",
    "\n",
    "        image = self.resize(image/image.max()).float().unsqueeze(1).to(device)\n",
    "\n",
    "        mask = torch.isnan(c)\n",
    "        c_mean = torch.nanmean(c,0)\n",
    "        c[mask[:,0],0] = c_mean[0]\n",
    "        c[mask[:,1],1] = c_mean[1]\n",
    "        c = c.long()\n",
    "        image = torch.stack([\n",
    "            image[\n",
    "                i,\n",
    "                :,\n",
    "                c[i,1]-patch_size//2:c[i,1]+patch_size-patch_size//2,\n",
    "                c[i,0]-patch_size//2:c[i,0]+patch_size-patch_size//2\n",
    "            ] for i in range(len(images))\n",
    "        ])\n",
    "        \n",
    "        if self.INFERENCE:\n",
    "            return image.to(device)\n",
    "        else:\n",
    "            indices = np.arange(len(images))\n",
    "            label_min = -torch.ones(5).int()\n",
    "            label_max = -torch.ones(5).int()\n",
    "            label = -torch.ones(5).float()\n",
    "            values_min = torch.as_tensor(row[['L1L2_min','L2L3_min','L3L4_min','L4L5_min','L5S1_min']].values.astype(int)).view(-1)\n",
    "            values_max = torch.as_tensor(row[['L1L2_max','L2L3_max','L3L4_max','L4L5_max','L5S1_max']].values.astype(int)).view(-1)\n",
    "            mask = values_min != 0\n",
    "            label_min[mask] = torch.as_tensor(np.array([np.where(instance_numbers==l)[0][0] if sum(instance_numbers==l)>0 else -1 for l in values_min[mask]]).astype(np.int32)).view(-1)\n",
    "            label_max[mask] = torch.as_tensor(np.array([np.where(instance_numbers==l)[0][0] if sum(instance_numbers==l)>0 else -1 for l in values_max[mask]]).astype(np.int32)).view(-1)\n",
    "            label[mask] = ((label_min + label_max)/2)[mask]\n",
    "            label = label*Lmax/D\n",
    "            if row['flip']:\n",
    "                image = image.flip(0)\n",
    "                c = c.flip(0)\n",
    "                label[label != -1] = Lmax - 1 - label[label != -1]\n",
    "            return image.to(device),label.to(device)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0aec33b59f4c209"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class Axial_ViT(nn.Module):\n",
    "    def __init__(self, ENCODER, dim=512, depth=24, head_size=64, **kwargs):\n",
    "        super().__init__()\n",
    "        self.ENCODER = ENCODER\n",
    "        self.AvgPool = nn.AdaptiveAvgPool2d(output_size=1).to(device)\n",
    "        self.slices_enc = nn.Parameter(SinusoidalPosEmb(dim)(torch.arange(Lmax, device=device).unsqueeze(0)))\n",
    "        self.slices_transformer = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True, device=device), depth)\n",
    "        self.proj_out = nn.Linear(dim,5).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ENCODER(x.view(-1,1,patch_size,patch_size))[-1]\n",
    "        x = self.AvgPool(x)\n",
    "        x = x.view(-1,Lmax,512)\n",
    "        x = x + self.slices_enc\n",
    "        x = self.slices_transformer(x)#,src_key_padding_mask=slices_mask)\n",
    "        x = self.proj_out(x.view(-1,512)).view(-1,Lmax,5).permute(0,2,1)\n",
    "#       MinMaxScaling along the class plane to generate a heatmap\n",
    "        min_values = x.min(-1)[0].view(-1,5,1)\n",
    "        max_values = x.max(-1)[0].view(-1,5,1)\n",
    "        x = (x - min_values)/(max_values - min_values)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83de60befd736dd7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6edcc3bcfb682f76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idx_map1 = torch.arange(Lmax).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a830765b1386892d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class myLoss1(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=.5\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def clone(self):\n",
    "        return myLoss(self.alpha)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            y,# Predictions\n",
    "            t # Targets\n",
    "        ):\n",
    "        available = t >= 0\n",
    "        mask_true = t[available]\n",
    "        mask_pred = y[available]\n",
    "#       The heatmap Loss as the distance between the predicted Normal and the ideal one\n",
    "#       Let's define the ideal heatmaps as the Normal distributions\n",
    "#       centered on the diagnostic centers with s2 = PATCH_SIZE/8\n",
    "        s2 = s2 = torch.as_tensor([PATCH_SIZE/16])\n",
    "#       Then the corresponding alphas and normalization constants would be\n",
    "        A = -1/(2*s2).to(device)\n",
    "        K = 1/torch.sqrt(2*math.pi*s2).to(device)\n",
    "#       Predicted heatmaps rescaling\n",
    "        mask_pred = mask_pred*K\n",
    "#       Ideal heatmaps\n",
    "        mask = mask_true.view(-1,1) - idx_map1.view(1,-1)\n",
    "        mask = mask*mask\n",
    "        mask = torch.exp(A*mask)*K\n",
    "\n",
    "#       Distance\n",
    "        D = 1 -((mask*mask_pred).sum(-1))**2/((mask*mask).sum(-1)*(mask_pred*mask_pred).sum(-1))\n",
    "        \n",
    "        return D.nanmean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c6e65d38d260b7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = level_train\n",
    "for f in range(1,6):\n",
    "    seed_everything(SEED)\n",
    "    seg_model = torch.load('./datasets/axial-seg-mobilnet/SEG_subarticular_'+str(f))\n",
    "    model = Axial_ViT(seg_model.UNet.encoder)\n",
    "    tdf = df[df.fold != f]\n",
    "    vdf = df[df.fold == f]\n",
    "    tds = Axial_ViT_Dataset(tdf,f)\n",
    "    vds = Axial_ViT_Dataset(vdf,f,VALID=True)\n",
    "    tdl = torch.utils.data.DataLoader(tds, batch_size=BS, shuffle=True, drop_last=True)\n",
    "    vdl = torch.utils.data.DataLoader(vds, batch_size=BS, shuffle=False)\n",
    "    dls = DataLoaders(tdl,vdl)\n",
    "\n",
    "    n_iter = len(tds)//BS\n",
    "\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        loss_func=myLoss1(),\n",
    "        cbs=[\n",
    "            ShowGraphCallback(),\n",
    "            GradientClip(3.0)\n",
    "        ]\n",
    "    )\n",
    "    learn.fit_one_cycle(EPOCHS, lr_max=1e-4, wd=0.05, pct_start=0.02)\n",
    "    torch.save(model,'axial_T2_levels_'+str(f))\n",
    "    del model,seg_model,tdf,vdf,tds,vds,tdl,vdl,dls,learn\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a31057f795f5f512"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "levels = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for X,l in tqdm(vdl):\n",
    "        levels = levels + torch.argmax((model(X)+model(X.flip(1)).flip(-1)),-1).cpu().tolist()\n",
    "        y_true = y_true + l.cpu().tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82a6fa18c6d7a0b2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_true = np.array(y_true).flatten()\n",
    "levels = np.array(levels).flatten()\n",
    "plt.plot(y_true,levels,'.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd3a7418b930625d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for X,l in tqdm(vdl):\n",
    "    with torch.no_grad():\n",
    "            levels = (model(X)+model(X.flip(1)).flip(-1)).cpu()/2\n",
    "            x0 = torch.argmax(levels,-1)\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ecbe2280b47953b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, x0, b):\n",
    "    r = x - x0\n",
    "    return np.exp(-b*r*r)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8dfed9f885ff563"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_meta_f = pd.read_csv('./datasets/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\n",
    "df_meta_f.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d9c6b17e69d5ba2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./datasets/rsna-2024-lumbar-spine-degenerative-classification/train.csv\")\n",
    "train.head()\n",
    "CV=5\n",
    "v,c = np.unique(train['study_id'],return_counts=True)\n",
    "L = len(v)\n",
    "S = L/CV\n",
    "fold_indices = list(np.rint(np.arange(CV)*S).astype(int))+[L]\n",
    "\n",
    "for i in range(5):\n",
    "    print(len(v[fold_indices[i]:fold_indices[i+1]]))\n",
    "    train.loc[train['study_id'].isin(v[fold_indices[i]:fold_indices[i+1]]),'fold'] = i+1\n",
    "train.tail()\n",
    "train = train[['study_id','fold']].merge(df_meta_f[df_meta_f.series_description == 'Axial T2'][['study_id','series_id']],left_on='study_id',right_on='study_id').sort_values('fold').reset_index(drop=True)\n",
    "train.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "225b8ccc9d4b4c7b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train = level_train[level_train['flip']==True]\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10349ddb81aead5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "indices = np.arange(Lmax)\n",
    "for f in [1,2,3,4,5]:\n",
    "    model = torch.load('./datasets/final-axial-slice-calssifier/New_Axial_classifier_'+str(f))\n",
    "    model.eval()\n",
    "    dl = torch.utils.data.DataLoader(Axial_ViT_Dataset(train,f,VALID=True,INFERENCE=True), batch_size=BS, shuffle=False)\n",
    "\n",
    "    popt = []\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(dl):\n",
    "            y_pred = (model(X)+model(X.flip(1)).flip(-1)).cpu()/2\n",
    "            x0 = torch.argmax(y_pred,-1)\n",
    "\n",
    "            for i in range(len(y_pred)):\n",
    "                for k in range(5):\n",
    "                    popt = popt + list(curve_fit(func, indices, y_pred[i][k],[x0[i][k],1],maxfev = 9999)[0])\n",
    "\n",
    "    \n",
    "    popt = torch.as_tensor(popt).view(-1,5,2).permute(0,2,1).reshape(-1,2*5)\n",
    "    mask = (popt[:,:5] < 0) + (popt[:,:5] > Lmax - 1)\n",
    "    popt[:,:5][mask] = torch.nan\n",
    "    popt[:,5:][mask] = torch.nan\n",
    "    train.loc[:,[\n",
    "        'L1L2_x0','L2L3_x0','L3L4_x0','L4L5_x0','L5S1_x0',\n",
    "        'L1L2_b','L2L3_b','L3L4_b','L4L5_b','L5S1_b'\n",
    "    ]] = np.array(popt)\n",
    "\n",
    "    train[[\n",
    "        'study_id','fold','series_id',\n",
    "        'L1L2_x0','L2L3_x0','L3L4_x0','L4L5_x0','L5S1_x0',\n",
    "        'L1L2_b','L2L3_b','L3L4_b','L4L5_b','L5S1_b'\n",
    "    ]].to_csv('Final_axial_T2_levels_'+str(f)+'.csv',index=False)\n",
    "    del model,dl,popt\n",
    "    gc.collect()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bed712652f4f62"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "v = train[['L1L2_b','L2L3_b','L3L4_b','L4L5_b','L5S1_b']].values\n",
    "plt.boxplot(v[~np.isnan(v)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d674ec4eb7bade9f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train[(train[['L1L2_b','L2L3_b','L3L4_b','L4L5_b','L5S1_b']]>np.percentile(v[~np.isnan(v)],99)).sum(1)>0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97c75526035f6df3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train[(train[['L1L2_b','L2L3_b','L3L4_b','L4L5_b','L5S1_b']]>np.percentile(v[~np.isnan(v)],99)).sum(1)>0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39a40e515c288f33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_train = train[(train[['L1L2_b', 'L2L3_b', 'L3L4_b', 'L4L5_b', 'L5S1_b']] <= np.percentile(v[~np.isnan(v)], 99)).all(1)]\n",
    "final_train = final_train[['study_id', 'series_id',\n",
    "       'fold', 'flip', 'L1L2_x0', 'L2L3_x0', 'L3L4_x0', 'L4L5_x0', 'L5S1_x0',\n",
    "       'L1L2_b', 'L2L3_b', 'L3L4_b', 'L4L5_b', 'L5S1_b']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "462a2f795d7b84f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_train.to_csv('final_slice_pred.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb6980de5977bab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Final Classifier Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a05b9f0415e434f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new_train_df = pd.read_csv(\"./datasets/data-to-train-on/data_to_train_on.csv\")\n",
    "new_train_df = new_train_df.dropna(axis=0).reset_index(drop=True)\n",
    "new_train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14791427d2d9fef4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "v = len(new_train_df)\n",
    "split_len = v/CV\n",
    "indices = list(np.rint(np.arange(CV)*split_len).astype(int))+[v]\n",
    "for i in range(CV):\n",
    "    new_train_df.loc[indices[i]:indices[i+1],'fold']=i+1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "366132b054b40ab8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'Normal/Mild':0,\n",
    "    'Moderate':1,\n",
    "    'Severe':2\n",
    "}\n",
    "coords_cols = ['x','y']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ef9a5d29276225b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment_image(image,alpha):\n",
    "    # Randomly rotate the image.\n",
    "    angle = torch.as_tensor(random.uniform(-180, 180)*alpha)\n",
    "    image = torchvision.transforms.functional.rotate(image.unsqueeze(0),angle.item())\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aafb5049c5f7ab00"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target = new_train_df.columns[-2]\n",
    "target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8018dce351aebf7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idx_map = torch.stack([torch.arange(PATCH_SIZE)]*PATCH_SIZE).to(device)\n",
    "idx_map = torch.stack([idx_map,idx_map.T]).view(1,1,2,PATCH_SIZE,PATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af943980401987c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class AxialUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myUNet, self).__init__()\n",
    "\n",
    "        self.UNet = smp.Unet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            classes=2,\n",
    "            in_channels=1\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self,X):\n",
    "        x = self.UNet(X)\n",
    "#       MinMaxScaling along the class plane to generate a heatmap\n",
    "        min_values = x.view(-1,2,PATCH_SIZE*PATCH_SIZE).min(-1)[0].view(-1,2,1,1) # Bug, I've been MinMaxScaling with the wrong values\n",
    "        max_values = x.view(-1,2,PATCH_SIZE*PATCH_SIZE).max(-1)[0].view(-1,2,1,1)\n",
    "        x = (x - min_values)/(max_values - min_values)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85ae3d6f0d9c5608"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ViT_T1_Dataset(Dataset):\n",
    "    def __init__(self, df, UNet, VALID=False, P=patch_size, alpha=0):\n",
    "        self.data = df\n",
    "        self.UNet = UNet\n",
    "        self.VALID = VALID\n",
    "        self.P = P\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        \n",
    "        sample = './datasets/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n",
    "        sample = sample+str(row['study_id'])+'/'+str(row['series_id'])+'/'+str(row['instance_number'])+'.dcm'\n",
    "        centers = torch.as_tensor([x for x in row[coords_cols]]).view(1,2).float()\n",
    "        image = pydicom.dcmread(sample).pixel_array\n",
    "        H,W = image.shape\n",
    "#       By plane resizing I've been distorting the proportions\n",
    "        if H > W:\n",
    "            d = W\n",
    "            h = (H - d)//2\n",
    "            image = image[h:h+d]\n",
    "            centers[:,1] -= h\n",
    "            H = W\n",
    "        elif H < W:\n",
    "            d = H\n",
    "            w = (W - d)//2\n",
    "            image = image[:,w:w+d]\n",
    "            centers[:,0] -= w\n",
    "            W = H\n",
    "        image = cv2.resize(image,(PATCH_SIZE,PATCH_SIZE))\n",
    "        image = torch.as_tensor(image/np.max(image)).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "        OUT = 0\n",
    "        with torch.no_grad():\n",
    "                for rot in [0,1,2,3]:\n",
    "                        OUT += torch.rot90(self.UNet(torch.rot90(image,rot,dims=[-2, -1])),-rot,dims=[-2, -1])\n",
    "#         print(OUT.shape)\n",
    "        OUT = (OUT/4 > 0.5)[0]\n",
    "        c = (OUT.unsqueeze(0)*idx_map[0]).view(1,2,PATCH_SIZE*PATCH_SIZE).sum(-1).float()\n",
    "        d = OUT.view(2,PATCH_SIZE*PATCH_SIZE).sum(-1).float()\n",
    "#         print(\"d \",d,d.shape)\n",
    "#         print(c.shape,d.shape) \n",
    "#         m = d > 0\n",
    "#         print(\"m\",m,m.shape)\n",
    "#         print(\"c[m]\",c[m],c[m].shape)\n",
    "        c = (c.squeeze(0)/d).float().unsqueeze(0)\n",
    "        c[torch.isnan(c)] = 256\n",
    "#         c = (c/d.unsqueeze(-1)).float()\n",
    "#         print(\"final C\",c,c.shape)\n",
    "#         c[~m] = self.P # I have to find a better solution\n",
    "#         c[~m] = 0\n",
    "#         return image\n",
    "#         c[c < 32] = torch.nan\n",
    "#         c[c > 580] = torch.nan\n",
    "#         c_mean = torch.nanmean(c, dim=0)\n",
    "#         mask = torch.isnan(c)\n",
    "#         c[mask[:,0],0] = c_mean[0]\n",
    "#         c[mask[:,1],1] = c_mean[1]\n",
    "        c = c.long()\n",
    "#         print(row['study_id'],row['series_id'],row['instance_number'])\n",
    "        image = torch.stack([image[\n",
    "            0,\n",
    "            0,\n",
    "            xy[1]-self.P//2:xy[1]+self.P-self.P//2,\n",
    "            xy[0]-self.P//2:xy[0]+self.P-self.P//2\n",
    "        ] for xy in c])\n",
    "#         print(image.shape,c,mask,c_mean)\n",
    "#         print(image.shape,c)\n",
    "        if not self.VALID: \n",
    "            \n",
    "            image = augment_image(image,self.alpha)\n",
    "#         label = torch.as_tensor([labels[x] for x in row[target]])\n",
    "        label = torch.as_tensor(labels[row[target]])\n",
    "       \n",
    "        return image.to(device),label.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a53d9acfe850ef27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class myAxialViT(nn.Module):\n",
    "    def __init__(self, dim=512, depth=12, head_size=128, **kwargs):\n",
    "        super().__init__()\n",
    "        CNN = torchvision.models.resnet18(weights='DEFAULT')\n",
    "        W = nn.Parameter(CNN.conv1.weight.sum(1, keepdim=True))\n",
    "        CNN.conv1 = nn.Conv2d(1, patch_size, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        CNN.conv1.weight = W\n",
    "        CNN.fc = nn.Identity()\n",
    "        self.emb = CNN.to(device)\n",
    "        self.pos_enc = nn.Parameter(SinusoidalPosEmb(dim)(torch.arange(1, device=device).unsqueeze(0)))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True, device=device), depth)\n",
    "        self.proj_out = nn.Linear(dim,3).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x.view(-1,1,patch_size,patch_size))\n",
    "        x = x.view(-1,1,512)\n",
    "        x = x + self.pos_enc\n",
    "        x = self.transformer(x)\n",
    "        x = self.proj_out(x.view(-1,512))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "929c9cdf112d7154"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def myLoss(preds,target):\n",
    "#     target,mask = target\n",
    "#     target = target[~mask]\n",
    "#     preds = preds[~mask.view(-1)]\n",
    "#     print(preds.shape,target.shape)\n",
    "    return nn.CrossEntropyLoss(weight=torch.as_tensor([1.,2.,4.]).to(device))(preds,target)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6499e89e3d2c2b2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def nt(nmin,nmax,tcur,tmax):\n",
    "    return (nmax - .5*(nmax-nmin)*(1+np.cos(tcur*np.pi/tmax))).astype(np.float32)\n",
    "\n",
    "plt.plot(nt(0,1,np.arange(10),10))\n",
    "plt.show()\n",
    "\n",
    "# callback to update alpha during training\n",
    "def cb(self):\n",
    "    alpha = torch.as_tensor(nt(.25,1,learn.train_iter,10*n_iter))\n",
    "    learn.dls.train_ds.alpha = alpha\n",
    "alpha_cb = Callback(before_batch=cb)#"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e611371473e1fcb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for fold in range(1,6):\n",
    "    seed_everything(SEED)\n",
    "    tdf = new_train_df[new_train_df['fold'] != fold]\n",
    "    vdf = new_train_df[new_train_df['fold'] == fold]\n",
    "    UNet = torch.load(\"./datasets/axial-seg-model/SEG_subarticular_\"+str(fold))\n",
    "    tds = ViT_T1_Dataset(tdf,UNet)\n",
    "    vds = ViT_T1_Dataset(vdf,UNet,VALID=True)\n",
    "    tdl = torch.utils.data.DataLoader(tds, batch_size=32, shuffle=True, drop_last=True)\n",
    "    vdl = torch.utils.data.DataLoader(vds, batch_size=32, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dls = DataLoaders(tdl,vdl)\n",
    "    \n",
    "    n_iter = len(tds)//32\n",
    "    model = myAxialViT()\n",
    "    \n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        lr=1e-4,\n",
    "        loss_func=myLoss,\n",
    "        cbs=[\n",
    "            ShowGraphCallback(),\n",
    "            alpha_cb,\n",
    "            GradientClip(3.0)\n",
    "        ]\n",
    "    )\n",
    "    learn.fit_one_cycle(10,lr_max=1e-4,wd=0.1)\n",
    "    torch.save(model,'VIT_Axial_Classifier_'+str(fold))\n",
    "    del tds,vds,tdl,vdl,dls,model,learn\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e22ab6493cb973"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
